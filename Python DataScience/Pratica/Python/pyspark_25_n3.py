# -*- coding: utf-8 -*-
"""pyspark_25_n3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1LVD0ba8AITtmCPLKXdrxYpWyig_vs_4C
"""

#criar ambiente de sessão spark em memoria

#instalando bibiloteca pyspark
!pip install pyspark

from pyspark.sql import SparkSession as ss
from pyspark.sql import functions as f
from pyspark.sql.types import DoubleType

#criando objeto spark
spark = ss.builder.appName("SENAI").getOrCreate()

spark

dados = (
    ("ana" ,  "analista" , 6200),
    ("joao" , "assistente", 3100),
    ("julio" , "analista", 7900),
    ("bruna" , "coordenadora", 16000),
    ("pedro" , "coordenador" , 15000),
    ("felipe" , "auxiliar" , 2500)
)

colunas = ("funcionarios" , "funcao" , "salario")

#criando data frame manual

df1 = spark.createDataFrame(dados,colunas)

df1.toPandas()

dados = (
    ("caludio","analista", 5800),
    ("beatriz","assistente",3300)
)

df2 = spark.createDataFrame(dados,colunas)

df2.show()

df = df1.union(df2)

df.show()

df.toPandas()

#exibir informaçeoes sobre tipos de dados e nomes de colunas do data frame
df.printSchema()

# adicionando nova coluna no data frame spark:

df.withColumn("cenario1",
              f.round(df.salario * 1.05)).show()

#efetivando a alteração do df
df = df.withColumn("cenario1",
              f.round(df.salario * 1.05))

df.toPandas()

#criando coluna condicional
#no novo cenario sera aplicado um reajuste para quem ganha
#ate 5000. para outro salario sera o reajuste padrão de 5%.

df.withColumn("cenario2",
              f.when(df.salario<= 5000, df.salario * 1.07).otherwise(df.salario * 1.05)).show()

df = df.withColumn("cenario2",
              f.when(df.salario<= 5000, df.salario * 1.07).otherwise(df.salario * 1.05))

df.toPandas()

#filtrando dados:
df.where("salario >= 7000").show()

df.where("funcao like '%coord%'").show()

#importando dataframe .csv
url= "/content/drive/MyDrive/BasesPark/empresas-20251002T220344Z-1-001/empresas/part*"
empresas = spark.read.csv(
    url,
    sep=";",
    header=True,
    inferSchema=True,
    encoding="latin1"
)

empresas.show()

empresas.printSchema()

# uso do toPandas com muitas linhas
empresas.limit(20).toPandas()

#qtd de registros
empresas.count()

empresas.show(truncate=False)

empresas.show(5,False)

#mostre alguns registros que tenhama palavra"hospital" em sua razaõ capital
#o'like' diferenci maiusculas de minusculas
#o 'ilike' não diferencia
empresas.where("razao_social ilike '%hospital%'").show(15, False)

#renomenado colunas :
empresas.withColumnsRenamed({
    "natureza_juridica" : "natureza",
    "qualificacao_responsavel" : "responsavel",
    "capital_social":"capital",
    "ente_federativo":"ente"
}).show()

#efetivando alteração:
empresas = empresas.withColumnsRenamed({
    "natureza_juridica" : "natureza",
    "qualificacao_responsavel" : "responsavel",
    "capital_social":"capital",
    "ente_federativo":"ente"
})

empresas.show()

#antes de converter a coluna capital teremos que subistituia a virgula por ponto
#caso contarrio o pyspark não consgue converter a coluna.. ela fica null

empresas.withColumn("capital",
                    f.regexp_replace("capital",",",".")).show()

empresas = empresas.withColumn("capital",
                    f.regexp_replace("capital",",","."))

# Importando dados de estabelecimentos:
url = "/content/drive/MyDrive/BasesPark/estabelecimentos-20251002T220343Z-1-001/estabelecimentos/part*"
estabs = spark.read.csv(
    url,
    sep=";",
    inferSchema=True,
    header=True,
    encoding="latin1"
)

#renomeando colunas
estabs.show(1)

estabs.withColumnsRenamed({
    "data_de_inicio_atividade":"data_inicio",
    "data_situacao_cadastral":"data_situação",
    "cnae_fiscal_principal":"cnae"
}).show(1)

estabs = estabs.withColumnsRenamed({
    "data_de_inicio_atividade":"data_inicio",
    "data_situacao_cadastral":"data_situação",
    "cnae_fiscal_principal":"cnae"
})

#convertendo coluna para tipo date
estabs.withColumn("data_inicio",
                  f.to_date("data_inicio","yyyyMMdd")).show(1)

estabs = estabs.withColumn("data_inicio",
                  f.to_date("data_inicio","yyyyMMdd"))

estabs.show(10)

# criando views para "codarmos" SQL bruto dentro da API pyspark

empresas.createOrReplaceTempView("v_empresas")
estabs.createOrReplaceTempView("v_estabs")

#função .sql() é respnsavel por enxergar as view e executar os comandos sql

spark.sql(
"""
SELECT *
FROM V_EMPRESAS A
WHERE A.RAZAO_SOCIAL LIKE '%SENAI%'
"""
).show(15,False)

spark.sql(
"""
SELECT *
FROM V_EMPRESAS A
INNER JOIN V_ESTABS B ON B.CNPJ_BASICO = A.CNPJ
"""
).show(15,False)

df = spark.sql(
"""
SELECT *
FROM V_EMPRESAS A
INNER JOIN V_ESTABS B ON B.CNPJ_BASICO = A.CNPJ
"""
)

df.show(3,False)

df.count()

url = "/content/drive/MyDrive/Colab Notebooks/SAIDA"

df.coalesce(1).write.orc(
    url,
    mode="overwrite"
)

# recriando data frame com o arquivo .orc gerado:
url = "/content/drive/MyDrive/Colab Notebooks/SAIDA/part*"
df = spark.read.orc(url)

df.show(3,False)

df.count()

#mosrtre as 50 primeiras empresas do dataframe
#ordene pela coluna data_inicio
df.select("cnpj","razao_social","nome_fantasia","data_inicio","uf")
df.orderBy("data_inicio").show(50,False)

#mostre todas as empresas fundadas entre 1901 e 1940
#mostre depois a quantidade delas:
df.select("cnpj","razao_social","nome_fantasia","uf","data_inicio").where("data_inicio between '1901-01-01' and '1940-12-31'").orderBy("data_inicio").show(50,False)
#
qtd = df.where("data_inicio between '1901-01-01' and '1940-12-31'").count()
print(f"Quantidade de empresas fundadas entre 1910 e 1940: {qtd:,.2f}")

df.select("cnpj","razao_social","nome_fantasia","uf","data_inicio")\
.where("municipio == 7097")\
.orderBy("data_inicio").show(50,False)
#

df.select("cnpj","razao_social","nome_fantasia","uf","data_inicio")\
.where("municipio == 7097")\
.orderBy("data_inicio", ascending=False).show(50,False)
#

# Mostre a quantidade total de empresas por estado.
df.where("capital >= 1000 and capital <= 100000000")\
.groupBy("uf")\
.agg(
    f.format_number(f.count("*"),2).alias("qtd_empresas"),
    f.format_number(f.sum("capital"),2).alias("valor_total"),
    f.format_number(f.mean("capital"),2).alias("media"),
    f.median("capital").alias("mediana"),
    f.stddev("capital").alias("desvio_padrao")
).orderBy("mediana", ascending=False).toPandas()

# Mostre a quantidade total de empresas por estado.
df.where ("municipio == 7097")\
.groupBy("bairro")\
.agg(
    f.count("*").alias("qtd_empresas"),
).orderBy("qtd_empresas", ascending=False).toPandas()

